# 第四阶段学习指南：理解 ML 模型

> **适合人群**：已完成第三阶段学习，对 Python/JAX 有基本了解  
> **预计时间**：5-7天  
> **目标**：深入理解 Phoenix 的 ML 模型（检索和排序）

---

## 📚 学习目标

完成第四阶段后，你应该能够：

1. ✅ 理解 Two-Tower 检索模型的架构和工作原理
2. ✅ 理解 Transformer 排序模型的架构和工作原理
3. ✅ 理解 Candidate Isolation 的机制和重要性
4. ✅ 理解多动作预测的设计
5. ✅ 能够阅读和理解 ML 模型代码

---

## 🎯 第一部分：检索模型（Two-Tower）

### 1.1 概述

**作用**：从百万级候选集中快速召回相关内容

**架构**：Two-Tower（双塔）架构
- User Tower：编码用户特征和历史
- Candidate Tower：编码所有帖子
- 相似度搜索：使用点积相似度

**输入输出**：
- 输入：用户特征、交互历史
- 输出：Top-K 候选帖子（例如：1000条）

### 1.2 Two-Tower 架构详解

#### 架构图

```
┌─────────────────────────────────────────────────────────┐
│                    TWO-TOWER MODEL                      │
├─────────────────────────────────────────────────────────┤
│                                                         │
│   ┌──────────────┐              ┌──────────────┐      │
│   │  USER TOWER  │              │ CANDIDATE    │      │
│   │              │              │ TOWER        │      │
│   │  用户特征    │              │              │      │
│   │  交互历史    │              │  所有帖子    │      │
│   │      ↓       │              │      ↓       │      │
│   │  Transformer │              │  Embedding   │      │
│   │      ↓       │              │      ↓       │      │
│   │  用户嵌入    │              │  帖子嵌入    │      │
│   │  [B, D]      │              │  [N, D]      │      │
│   └──────┬───────┘              └──────┬───────┘      │
│          │                              │              │
│          └──────────┬───────────────────┘              │
│                     │                                   │
│                     ▼                                   │
│            ┌─────────────────┐                          │
│            │  相似度搜索     │                          │
│            │  (点积)        │                          │
│            └─────────────────┘                          │
│                     │                                   │
│                     ▼                                   │
│            Top-K 候选帖子                               │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

#### User Tower（用户塔）

**作用**：将用户特征和交互历史编码为嵌入向量

**输入**：
- 用户特征（关注列表、偏好等）
- 交互历史（最近点赞、转发、回复的帖子）

**处理**：
- 使用 Transformer 编码
- 输出归一化的用户嵌入 `[B, D]`

**代码位置**：`phoenix/recsys_retrieval_model.py`

#### Candidate Tower（候选塔）

**作用**：将所有帖子编码为嵌入向量

**输入**：
- 所有帖子的特征

**处理**：
- 计算嵌入向量
- 输出归一化的帖子嵌入 `[N, D]`（N = 所有帖子数量）

**特点**：
- 可以预先计算（离线）
- 存储在向量数据库中

#### 相似度搜索

**方法**：点积相似度

```
相似度 = user_embedding · candidate_embedding
```

**实现**：
- 使用近似最近邻（ANN）搜索
- 例如：FAISS、Annoy 等

**输出**：Top-K 最相似的帖子

### 1.3 代码阅读指南

**文件位置**：`phoenix/recsys_retrieval_model.py`

**关键函数**：

1. **`RecsysRetrievalModel`**：主模型类
   - `__call__`：前向传播
   - `encode_user`：编码用户
   - `encode_candidates`：编码候选

2. **`block_user_reduce`**：组合多个用户哈希嵌入
   - 输入：多个哈希嵌入
   - 输出：单一用户表示

3. **`block_candidate_reduce`**：组合多个候选哈希嵌入
   - 输入：多个哈希嵌入
   - 输出：单一候选表示

### 1.4 任务清单

- [ ] 阅读 `phoenix/recsys_retrieval_model.py`
- [ ] 理解 User Tower 的实现
- [ ] 理解 Candidate Tower 的实现
- [ ] 理解相似度搜索的原理
- [ ] 运行 `uv run run_retrieval.py`，观察执行流程
- [ ] 阅读 `phoenix/test_recsys_retrieval_model.py`，理解测试用例

---

## 🧠 第二部分：排序模型（Transformer）

### 2.1 概述

**作用**：对检索到的候选进行精确排序

**架构**：基于 Grok-1 的 Transformer

**关键特性**：
- Candidate Isolation（候选隔离）
- 多动作预测
- Hash-Based Embeddings

**输入输出**：
- 输入：用户上下文 + 候选帖子
- 输出：每个候选的多个动作概率

### 2.2 Transformer 架构详解

#### 架构图

```
┌─────────────────────────────────────────────────────────────┐
│              PHOENIX RANKING MODEL                           │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│                    输出逻辑值                                │
│            [B, num_candidates, num_actions]                 │
│                      │                                       │
│                      │ 反嵌入投影                            │
│                      │                                       │
│          ┌───────────┴───────────┐                           │
│          │                       │                           │
│          │    Transformer        │                           │
│          │  (带特殊掩码)         │                           │
│          │                       │                           │
│          │  候选之间不能相互关注 │                           │
│          │                       │                           │
│          └───────────┬───────────┘                           │
│                      │                                       │
│    ┌─────────────────┼─────────────────┐                   │
│    │                 │                 │                   │
│    ▼                 ▼                 ▼                   │
│ ┌────────┐    ┌──────────────┐    ┌────────────┐          │
│ │ 用户   │    │   历史       │    │  候选      │          │
│ │嵌入    │    │  嵌入        │    │  嵌入      │          │
│ │[B, 1]  │    │  [B, S, D]   │    │  [B, C, D] │          │
│ └────────┘    └──────────────┘    └────────────┘          │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

#### 输入编码

**用户嵌入**：
- 输入：用户特征（哈希）
- 输出：用户嵌入 `[B, 1, D]`

**历史嵌入**：
- 输入：用户最近的交互历史
  - 帖子 + 作者 + 动作类型 + 产品表面
- 输出：历史嵌入 `[B, S, D]`（S = 历史长度，例如：128）

**候选嵌入**：
- 输入：候选帖子
  - 帖子 + 作者 + 产品表面
- 输出：候选嵌入 `[B, C, D]`（C = 候选数量，例如：32）

#### Transformer 层

**架构**：基于 Grok-1
- 多头自注意力（Multi-Head Self-Attention）
- 前馈网络（Feed-Forward Network）
- 层归一化（Layer Normalization）

**关键**：特殊的注意力掩码（Candidate Isolation）

### 2.3 Candidate Isolation（候选隔离）

#### 什么是 Candidate Isolation？

**定义**：在 Transformer 推理时，候选之间不能相互关注，只能关注用户和历史。

**为什么需要？**

1. **分数一致性**：确保候选的分数不依赖于批次中的其他候选
2. **可缓存性**：可以缓存候选的分数，提高性能
3. **公平性**：每个候选独立评估，不受其他候选影响

#### 注意力掩码可视化

```
                   注意力掩码可视化

         Keys (我们关注的内容)
         ─────────────────────────────────────────────▶

         │ 用户 │    历史 (S 个位置)    │   候选 (C 个位置)    │
    ┌────┼──────┼─────────────────────────┼───────────────────────┤
    │    │      │                         │                       │
    │ U  │  ✓   │  ✓   ✓   ✓   ✓   ✓   ✓  │  ✗   ✗   ✗   ✗   ✗   ✗    │
    │    │      │                         │                       │
    ├────┼──────┼─────────────────────────┼───────────────────────┤
 Q  │    │      │                         │                       │
 u  │ H  │  ✓   │  ✓   ✓   ✓   ✓   ✓   ✓  │  ✗   ✗   ✗   ✗   ✗   ✗    │
 e  │ i  │  ✓   │  ✓   ✓   ✓   ✓   ✓   ✓  │  ✗   ✗   ✗   ✗   ✗   ✗    │
 r  │ s  │  ✓   │  ✓   ✓   ✓   ✓   ✓   ✓  │  ✗   ✗   ✗   ✗   ✗   ✗    │
 i  │ t  │  ✓   │  ✓   ✓   ✓   ✓   ✓   ✓  │  ✗   ✗   ✗   ✗   ✗   ✗    │
 e  │    │      │                         │                       │
 s  ├────┼──────┼─────────────────────────┼───────────────────────┤
    │    │      │                         │  仅对角线（自关注）    │
 │  │ C  │  ✓   │  ✓   ✓   ✓   ✓   ✓   ✓  │  ✓   ✗   ✗   ✗   ✗   ✗    │
 │  │ a  │  ✓   │  ✓   ✓   ✓   ✓   ✓   ✓  │  ✗   ✓   ✗   ✗   ✗   ✗    │
 │  │ n  │  ✓   │  ✓   ✓   ✓   ✓   ✓   ✓  │  ✗   ✗   ✓   ✗   ✗   ✗    │
 │  │ d  │  ✓   │  ✓   ✓   ✓   ✓   ✓   ✓  │  ✗   ✗   ✗   ✓   ✗   ✗    │
 │  │ i  │  ✓   │  ✓   ✓   ✓   ✓   ✓   ✓  │  ✗   ✗   ✗   ✗   ✓   ✗    │
 │  │ d  │  ✓   │  ✓   ✓   ✓   ✓   ✓   ✓  │  ✗   ✗   ✗   ✗   ✗   ✓    │
 ▼  │ s  │  ✓   │  ✓   ✓   ✓   ✓   ✓   ✓  │  ✗   ✗   ✗   ✗   ✗   ✗    │
    │    │      │                         │                       │
    └────┴──────┴─────────────────────────┴───────────────────────┘

    ✓ = 可以关注 (1)          ✗ = 不能关注 (0)

    规则：
    ├─ 用户 + 历史：它们之间完全双向关注
    ├─ 候选 → 用户/历史：候选可以关注用户和历史  
    └─ 候选 → 候选：候选之间不能相互关注（仅自关注）
```

#### 代码实现

**文件位置**：`phoenix/recsys_model.py`

**关键函数**：构建注意力掩码

```python
def build_attention_mask(
    history_len: int,
    candidate_len: int,
) -> jnp.ndarray:
    """
    构建注意力掩码，实现 Candidate Isolation
    
    返回：[1 + history_len + candidate_len, 1 + history_len + candidate_len]
    """
    total_len = 1 + history_len + candidate_len
    
    # 初始化掩码（全1，表示可以关注）
    mask = jnp.ones((total_len, total_len))
    
    # 候选不能关注其他候选（除了自己）
    candidate_start = 1 + history_len
    candidate_end = total_len
    
    # 设置候选之间的掩码为0（除了对角线）
    for i in range(candidate_start, candidate_end):
        for j in range(candidate_start, candidate_end):
            if i != j:  # 不是自己
                mask = mask.at[i, j].set(0)
    
    return mask
```

### 2.4 多动作预测

#### 什么是多动作预测？

**定义**：模型同时预测多个交互类型的概率，而不是单一的"相关性"分数。

**预测的动作**：

**正面动作**（正权重）：
- `favorite`：点赞
- `reply`：回复
- `retweet`：转发
- `quote`：引用
- `click`：点击
- `profile_click`：点击个人资料
- `video_view`：观看视频
- `photo_expand`：展开照片
- `share`：分享
- `dwell`：停留
- `follow_author`：关注作者

**负面动作**（负权重）：
- `not_interested`：不感兴趣
- `block_author`：屏蔽作者
- `mute_author`：静音作者
- `report`：举报

#### 输出格式

```
输出: [B, num_candidates, num_actions]

例如：
候选1: [0.8, 0.3, 0.5, 0.2, ...]  # 点赞概率0.8，回复概率0.3，...
候选2: [0.6, 0.4, 0.7, 0.1, ...]
...
```

#### 为什么需要多动作预测？

1. **更丰富的信息**：不同动作反映不同的用户意图
2. **灵活的权重**：可以根据业务需求调整权重
3. **更好的解释性**：可以理解用户可能采取的具体动作

### 2.5 Hash-Based Embeddings

#### 什么是 Hash-Based Embeddings？

**定义**：使用多个哈希函数进行嵌入查找，而不是直接使用 ID。

**优势**：
- 减少嵌入表大小
- 处理未见过的 ID（OOV）
- 提高泛化能力

#### 实现方式

```python
# 使用多个哈希函数
num_hashes = 2  # 例如：2个哈希函数

# 对每个ID，计算多个哈希值
hash_values = [hash1(id), hash2(id)]

# 查找多个嵌入
embeddings = [embedding_table[hash1(id)], embedding_table[hash2(id)]]

# 组合多个嵌入
combined_embedding = reduce(embeddings)  # 例如：平均或投影
```

### 2.6 代码阅读指南

**文件位置**：`phoenix/recsys_model.py`

**关键类和函数**：

1. **`PhoenixModel`**：主模型类
   - `__call__`：前向传播
   - `_get_action_embeddings`：动作嵌入
   - `_build_input_embeddings`：构建输入嵌入

2. **`block_user_reduce`**：组合用户哈希嵌入

3. **`block_candidate_reduce`**：组合候选哈希嵌入

4. **`build_attention_mask`**：构建注意力掩码（实现 Candidate Isolation）

### 2.7 任务清单

- [ ] 阅读 `phoenix/recsys_model.py`
- [ ] 理解 Transformer 架构
- [ ] 理解 Candidate Isolation 的实现
- [ ] 理解多动作预测
- [ ] 理解 Hash-Based Embeddings
- [ ] 运行 `uv run run_ranker.py`，观察执行流程
- [ ] 阅读 `phoenix/test_recsys_model.py`，理解测试用例

---

## 🔬 第三部分：模型训练和推理

### 3.1 训练流程（概述）

**注意**：本项目主要关注推理，训练流程不在本仓库中。

**训练数据**：
- 用户交互历史
- 正样本：用户实际交互的帖子
- 负样本：用户未交互的帖子（随机采样）

**训练目标**：
- 最大化正样本的概率
- 最小化负样本的概率

### 3.2 推理流程

#### 检索阶段

```
1. 编码用户
   user_embedding = user_tower(user_features, history)

2. 相似度搜索
   similarities = user_embedding @ candidate_embeddings.T
   top_k_indices = top_k(similarities, k=1000)

3. 返回 Top-K 候选
   candidates = candidate_embeddings[top_k_indices]
```

#### 排序阶段

```
1. 准备输入
   - 用户嵌入
   - 历史嵌入（用户最近的交互）
   - 候选嵌入（检索到的候选）

2. Transformer 推理
   - 应用注意力掩码（Candidate Isolation）
   - 通过 Transformer 层
   - 输出逻辑值

3. 提取预测
   - 提取候选位置的输出
   - 转换为概率（softmax）
   - 返回多个动作的概率
```

### 3.3 性能优化

#### 检索优化

1. **预计算候选嵌入**：离线计算所有帖子的嵌入
2. **向量数据库**：使用 FAISS、Annoy 等快速搜索
3. **批量处理**：批量编码用户

#### 排序优化

1. **批量推理**：一次处理多个候选
2. **缓存**：缓存用户嵌入和历史嵌入
3. **量化**：使用 bfloat16 等低精度

### 3.4 任务清单

- [ ] 理解检索的推理流程
- [ ] 理解排序的推理流程
- [ ] 理解性能优化的方法
- [ ] 运行代码，观察推理时间

---

## ✅ 第四步：自我检查

### 检查清单

完成以下检查，确保你理解了：

#### 检索模型
- [ ] 我能解释 Two-Tower 架构吗？
- [ ] 我能解释 User Tower 如何工作吗？
- [ ] 我能解释 Candidate Tower 如何工作吗？
- [ ] 我能解释相似度搜索吗？

#### 排序模型
- [ ] 我能解释 Transformer 架构吗？
- [ ] 我能解释 Candidate Isolation 吗？
- [ ] 我能解释为什么需要 Candidate Isolation 吗？
- [ ] 我能解释多动作预测吗？

#### 代码理解
- [ ] 我能阅读和理解检索模型代码吗？
- [ ] 我能阅读和理解排序模型代码吗？
- [ ] 我能理解注意力掩码的实现吗？

---

## 🎓 实践练习

### 练习1：运行模型

1. **运行检索模型**：
   ```bash
   cd phoenix
   uv run run_retrieval.py
   ```

2. **运行排序模型**：
   ```bash
   uv run run_ranker.py
   ```

3. **运行测试**：
   ```bash
   uv run pytest test_recsys_model.py test_recsys_retrieval_model.py
   ```

### 练习2：理解注意力掩码

1. 画出注意力掩码的可视化图
2. 理解每个位置的掩码值
3. 理解为什么候选之间不能相互关注

### 练习3：修改模型参数

1. 修改历史长度（`history_seq_len`）
2. 修改候选数量（`candidate_seq_len`）
3. 观察对性能的影响

---

## 📝 学习笔记模板

```
# 第四阶段学习笔记

## 日期：____

## 检索模型（Two-Tower）
User Tower：
[你的理解]

Candidate Tower：
[你的理解]

相似度搜索：
[你的理解]

## 排序模型（Transformer）
架构：
[你的理解]

Candidate Isolation：
[你的理解]

多动作预测：
[你的理解]

Hash-Based Embeddings：
[你的理解]

## 代码理解
检索模型代码：
[你的理解]

排序模型代码：
[你的理解]

## 不懂的地方
[记录不懂的地方]

## 收获
[记录学到的知识]
```

---

## 🚀 下一步

完成第四阶段后，你应该：

1. ✅ 理解 ML 模型的架构
2. ✅ 理解 Candidate Isolation 的重要性
3. ✅ 能够阅读和理解模型代码

**准备好进入第五阶段了吗？**

第五阶段将进行：
- 实践和实验
- 性能优化
- 功能扩展

---

**祝你学习顺利！🎉**

记住：理解 ML 模型是理解推荐系统的核心，多运行代码，多实验！
